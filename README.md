# VEHICLE-INSURANCE-DOMAIN

Project Introduction 



--------------------------------------------------------------------------------------------------------------
                                               Project Introduction 
**Tools and Techniques i used in this project ->**
*MongoDB - for data storage
*Robust pipeline - Training/Prediction
*FastApi - For high performance prediction API ( acts as a waiter sits inside a docker which is stored on ECR)
*CICD - Github Actions
*AWS - IAM,ECR,EC2(IAM for managing permissions securely,ECR(Elastic Container Registry - to store docker images ,EC2(Elastic Compute Cloud - a cloud to host and run your application(i.e FastAPI inside a Docker container)
*No conventional mlops tools used ( i.e no Kubeflow , MLflow or sagemaker)

overview - 
FastApi is the waiter,its a python framework that creates an API 
you sent an HTTP request to specific URL and FastAPI receives the order and carries it to the ML model
Then the Fast API takes this answer and delivers it back to as an HTTP response

ECR is like an App store that just stores the Docker image
EC2 is a virtual computer that we rent from AWS,EC2 instance is a blank computer

**Discussing System Design**
Components -> Data Ingestion,Data Validation,Data Transformation,Model Training,Model Evaluation,Model Pusher
Data Ingestion - attaching multiple files,connects to mongoDb and fetches desired data
Data Validation - check columns, consistency etc
Data Transformation - feature engineering
Model training - Trian the model
Model Evaluation - adjusting paramenters , Hypertuning
Model Pusher - push the model to cloud

**Workflow**
constants->config_entity->artifact_entity->component->pipeline->demo.py/app.py

--------------------------------------------------------------------------------------------------------------
                                            MONGO DB SETUP
I now created a GitHub repo with .gitignore(python) , ReadMe.md(the birth of this file :D) , Liscence(MIT)
in Local Computer i opened my code editor( i use vscode) and cloned the Github.
now in the project folder i create a virtual python environment(named mlops) and source/bin/activate it :D
i add the virtual environment to .gitignore as venvs are huge storage and often not pushed to github
I create a template.py which contains the fodler structure of mlops project i acquire this from internet as folder structures are often common i run this python script to create empty folder and files the template.py is also pushed on github

Now the official project begins
i began with writing setup.py and pyproject.toml
But First what are they?
lets start with setup.py
setup.py - This is the original,classic method for creating a package.Its a python script that uses the setuptools library
Why use it? - Its a script which makes it extremely powerful and flexible

explanation of the script
name = 'src' -> it tells how pip sees your project ex pip install <name>
version -> the version number important for tracking changes
author and author_email -> simple metadata
packages=find_packages() -> it tells setup.py what code to include in the package in our case find_packages() automatically finds all folders that have an __init__.py file in our case src and all its subfolders contain __init__.py so it includes all those code

now about pyproject.toml(the modern way) ->It's the modern standard. It not only defines your package but also configures all your development tools toml stands for (toms obvious minimal language)
from file we can see  [tool.setuptools] -> section tells your build tool
packages={find={}}->this line is the new,declarative wat to say find_packages()
[tool.setuptools.dynamic] -> this is for settings that are dynamically loaded from other files
dependencies = {file = "requirements.txt} -> this is a key feature it tells setuptools to find the dependencies(like pandas ,fastapi) go and read the requirements.txt file 

WHy do we have BOTH?
mine pyproject.toml file is complete it has all the infor from setup.py and more(like dependencies part)
when we have both modern pip and other setuptools will prefer pyproject.toml
the setup.py is not often kept for backward compatibility or for complex logix that pyproject.toml cant handle for this project it's reduntant

Now i added requirements.txt file to our project root folder
it contains the following libraries
#Data Science and ML
pandas -> for data manipulation and analysis(think it as excel sheer of python)
numpy -> the fundamental package for scientific computing and working with arrays
matplotlib -> a standard library for creating static plots and graphs
plotly -> for creating modern interactive plots
seaborn -> built on top of matplotlib,makes creating beautiful statistical plots easier
scikit-learn -> this the machine learning library(ofc everyone knows it)
imblearn -> a spl sklearn add on to handle imbalanced datasets(like 99% no fraud and 1% fraud samples)
#API & Web Server
fastapi -> the waiter we use to build high speed prediction api
uvicorn -> the web server that runs the fastAPI application
python-multipart -> required by FastAPI to handle file uploads
jinja2 -> a tempalting engine(allows api to serve anHTML webpage like a demo dashboard)
#Database & Cloud
pymongo -> the official driver to connect to and interact with mongoDB database
boto3 -> the official AWS SDK for python allows code to talk with AWS services like S3 EC2 ECR
botocore -> the low level library that boto3 is built on
mypy-boto3-s3 -> provides type-hinting for boto3(auto completiomn and error checking)
#Core project & Utilities
ipykernel -> the kernel allows Jupyter notebooks to run your mlops virtual env
from_root -> a small utility to manage file paths ,making easy to find files relative to projects root directory
dill -> an advanced version of Python's pickle used to serialize(save) complex python objects like trained ML models
certifi -> orivides an up to date bundle of SSL certificates which pymongo and other web libraries need to make secure (httpS) connections
pyYAML -> a lib for reading and writing .yaml files 

-e . -> is an instructin for pip tells pip to install the project in the current directory(.) in editable mode(-e) this command that installs my src folder inside my mlops virtual env

The command pip install -e . is crucial for our project because it installs our src folder as a locally editable package within our virtual environment. The primary problem this solves is the ModuleNotFoundError that would otherwise happen when scripts outside src (like app.py) try to import code from inside it. Python doesn't automatically know to look in the src folder, but running this command using our pyproject.toml file fixes that. It creates a link in Python's main site-packages directory that points directly to our src folder, effectively teaching Python to treat src just like any other installed library, such as pandas or numpy. This makes our project's code importable from anywhere. The -e (or "editable") flag is the most important part, as it installs a link instead of a copy. This means any edits we make to our source code are immediately effective on the next run, with no re-installation required. This is the standard, clean way to make our project's modules available during development.

when you type pip list it shows currently installed packages i.e. for me it showed just pip but after running pip install -r requirements.txt pip will install all the listed librariesp
and when you type pip list all the libararies and their dependencies show up but here a point to notice is that in the list we can also see src show up this is due to -e . part instruction

here comes the database part where i setup mongoDB organisation and then i create a project inside that project i create a cluster i used email to login into MongoDB atlas, the project name is vehicle_proj, now we create a cluster we use free plan as for now and then select mumbai server and create a cluster we get a username and password that are required to gain access to that cluster save that username and password somewhere in the notepad now go to network access and add ip address so that we can access it from anywhere i.e., 0.0.0.0/0 now we head back to project site and click on get connection string now we select python as the driver and Version as 3.12 and later as it is a stable version now copy the connection string and paste it inside your project folder now i create a folder called notebook which contains data.csv file of our project also now i create a file called monoDB_demo.ipynb this is for pushing dataset folder from local to mongoDB cloud inside mongoDB_demo file we import pandas and pymongo libraries we create a df of data.csv and use head() function to check first five rows of dataset, now df should be converted in to dict before we push it to mongoDB as mongoDB is no relational database so we should upload it into key-value pair we use to_dict(orient='records') function for it now our next step is to setup cloud database with DB_NAME = 'Proj1' and COLLECTION_NAME='Proj1-Data' CONNECTION_URL = "the url that you saved in notepad no i am not dumb enough to put it here in a public repository" now we use pymongo.MongoClient to create a client object which is used to connect to entire MongoDB server(cluster) and use that client object to connect to the data_base.I encountered a problem here where i used @ in password for database and struggled for 2 hours but finally debugged it yeah a simple mistake and now i pushed the data.csv to mongoDB cluster using collection.insert_manY(data), we can see the uploaded data in mongoDB atlas and see all the collection/documents.

Now i move on to the next part i.e., Logging of the data so now go to src folder and open logger folder and open init.py Now logging seems to be kinda confused for me as i am new to this concept but repetitive patterns have enables me to understand few things,we use logging so that when the terminal of program ran is closed the logs are still saved(unlike print statements) and also logging is classified by importance DEBUG,INFO,WARNING,ERROR so coming to the coding part logging is the main python librarry for logging activity ,os is used to build file paths RotatingFileHandler is a special Editor Instead of writing to one giant file forever,it rotates the log file once it reaches a certian size i.e, MAX_LOG_SIZE (in this case) it renames app.log(old) to app.log.1 and writes again into app.log this continous till the BACKUP_COUNT limit has reaches and then the oldest log is deleted to create room for new logs .from_root is the helper library i installed to find the projects main folder and datetime is used to create a unique timestamped file name for the log we now delcare the constants for logs and create the log paths using os and constants  now we use logging library to get logger object from getLogger function and set its logging level to debug now we create a formatter variable that holds the pattern of how logs should be stored  now we create a file_handler object that creates log files  and then we create a console_handler object that puts logs into console now we add those both handlers to the logger object NOte that logs folder that is created by logger is not pushed to github as it is present in .gitignore for anytime we want to maintain logs of a python script file we import logging module from logger folder even tho there is no such functin called logging the script wil import entire logging module which will contain logger object

Now we move on to the next part i,e., handling exceptions of the data so now go to src folder and open exception folder and open init.py this file contains the folowing items error_message_detail(a helper function) which takes two arguments Exception itself(from the except block of script) and sys module this function only purpose is to log details of exception this details include filename line number amd the file also contains custom Exception class called MyException this contains __init__ function first it calls super() i.e., actual Exception class so that the except block behaves normally and then it calls error_message_detail which is the helper function that logs error_message and returns it so that it can be used by another mehtod in MyException class __str__ which is used to print error_message anytime we want from main script where exception has raised.Note that in the helper function we use error_detail.exc_info() which return three tuples Exception Type,Exception Value,Traceback we dont care about the first two we onlt want the traceback object(exc_tb) which holds all the call stack information now file_name = exc_tb.tb_frame.g_code.co_filename this digs into the traceback object(exc_tb) to find the frame(tb_frame) then the code(f_code_ and finally its filename(co_filename) and exc_tb.lineno pulls the line number directly from the trackback object all this information is stored in a sdetailed string called error_message and then we use logging to log the error.you will never call error_message_detail directly you wil only use MyExeption inside a try..exxcept block.

Coming up to the final part of this section and most important part creating the experiment-notebook i.e., exp-notebook.ipynb create it in the notebook folder and i import all the essentials i.e., numpy pandas matplotlib seaborn imblearn and sns ,warnings i create a dataframe df of data.csv and use head() now lets get into the data EDA part where id stands for unique id of the customer gender is gender age is age and driving_liscence where 1 represent they poses it and 0 represents they do not posses it and region_code that is unique code for the reguion of the customer previously_insured where 1 represent customer already has vehicle insurance 0 represent customer doesnot have behicle insureance ,vehicle_age,vehicle_damage where 1 means customer got his behicle damaged in tehe past 0 means customer doesnothave behicle damage in the past , annual_premium the amount customer needs to pay as premium in the year ,policy sales channel : anonymized code for the channel of outreaching to the customer i.e., different agents over mail , phone,person .VIntage: no of days customer has been associated with the company , Response where 1 is customer is interested 0 means cistomer is not insterested i use df.shape to find that there are 381109 rows in data i check ofr isnull and find that there are no null values in the data and i use df.info to find that there are three objects in the data(gender Male Female,Vehicle_Age 2 years,3 years etc,Vehicle_damage Yes or No which are objects now i use value_counts() on response feature to find that there are 334399 who dont want insurance and 46710 who want it i then use histogram to find that there are lot of young people the in dataset than old people then i use scatter plot for age vs annual premium and see that there is an almost equal situation and then i see gender distribtution using value_counts and plt and see that there are almost same number of male and females with male leading by few margin and then i create a response graph of males and females and see that male took more response than females and then i see plot of Gender and see that male have more drivingLiscense and Female have less id and then i keep on doing some EDA until i find that vehicles with young age have high amount of premium payers than vehicles with age > 2 years and then i also find that people who have their vehicle damaged in past prefer to take insurance than people who do not had their vheiicle damaged i also find that majority of the people pay 1-100000 and very les ppl pay 200000 DATA PREPROCESSING PART -> Now our goal is to convert all the data to numerical i.e., ogjects to numbericals the simple one is Gender and i used manual encoding(mapping) to convert gender to 0 and 1 as then convert them into int to know what features are numerical and what are categorical we can just do a simple test of adding,subracting or avaeraging the numbers and the result has a real world meaning its a numerical feature now out next object was  Vehicle_Age we used Automatic Encoding(One-hot encoding) its a one shot comman that automatically finds all other remaining object columns and converts them we used pd.get_dummies which creates new columsn for each category and we used drop_first = True as it orevents something caleld the dummy variable trap where if a car is NOT < 1 year and Car is NOT > 2 year then it must be 1-2 years so we drop that column of 1-2 year and then we clean up the created column names of get_dummies and make the results into int Now we move on to next part of scaling the data which is the critical step before training most machine learning models we do it because our features have huge scale Age is from 20 to 80 Vintage is from 10 to 300 and Annual_premium can be from 2600 to 300000 many ML algorithms work by calculating distances To these algorithms the huge gap in numbers might cause inaccurate training as the number 30000 fir annual premium may seem w ay more important than the number 26000 just because its bigger this can lead to ANnual_premium feature completely dominating the models decisions and Age feature would be ignoredScaling fixes this it puts all teh features on a level playing field (bw 0 to 1 or some thing else) so the model can judge them fairly based on the predictive power not their arbitrary scale i used two different Scalers one is standard Scaler (for age and vintage) the funciton of SS is it transform the data so that it has a mean of 0 and standard deviation of 1 which means when the value of a feature is 0 it meansexactly average and value of 1.5 means 1.5 standard deviations above average SS is a robust default choice that works very well for features that are normally distrubitued like Age and the other one MinMaxScaler (for annual premium) does squash all the data to fit within a specific range by default between0 and 1 which results in lowest premium being 0 and highest premium being 1 this strictly caps the infleunce of ANnual_Premium column which has a very large range and then after Transforming we save the column id to a variable id and then drop id from main DataFrame df which is a unique identifier and has zero predictive value we save it because its a good practice i might need the id list later to match models predictions bakc to the original customers in my database now we move on to next important step TRAINING THE MODEL before training it we split the data into train_test_split using X_trian,X_test,y_train,y_test and then we use Random Forest Algorithm which is a collection of random decision trees as we kniw decision tree  is highly sensitive to traininig data even a small amount of training data can lead to huge changes in the tree,so a random forest tree is often stable as it works on different concept its called a tree because it is a collection of deicicion trees at first we divide the main dataset into various datasets where each dataset contains random rows this process is called bootsrapping but heres twist we dont use every feature for that sub datasets we will randomly selected features and create decision trees and now prediction is made for a datapoint through each tree and note down the predictions of each tree and then a majority voting is done to select the final output this process of comiobinng  results from multiple trees is called Aggregation and together(aggregation and bootsrapping) is called bagging.Researchs has found that Usually no of features for each tree should be square root of total no of features.In machine learning there are two types of paraments Model Parameters and Hyperparameters where model parameters are those of the model that can be training with training data these can be considered as internal parameters(Ex - Weights and Bias) and Hyperparameters are whose values control the learning process there are adjustable parameters used to obtain  an optimal model aka External Parameters which can be controlled  by us(learning rate ,number of Epocks,n_estimators).So fundamentally speaking Hyperparameter tuning is a process of finding Best Hyperparameters and model training is a process for best model parameters
RandomizedSearchCV is a technique for hyperparameter tuning for a RandomForest the hyperparameters are n_estimators which means how many individual decision trees to build in your forest usually it is set at 300 ,max_depth the deepest ant single tree can go i,e., max depth of tree,min_samples_split :the minimum number of data points required in a node before the tree is allowed to split if the points are less than it then it is considered as leaf node,min_samples_leaf the minimum number of data points that mmust be in a final leaf after splitting and this check happens after a split is proposed if the criteria is not met then the split does not take place and the node becomes the leaf node  now min_samples_split is the first check that happens and then min_samples_leaf is the second check Increasing both of these values makes your model simpler and more general, which is the primary way to fight overfitting, crtierion : entropy or gini the mathematical formula used to measure the purity or information gain of a question Whereas in RandomizedForest Search the hypermparameters are estimator which means the model that we use , param_distributions : the dictionary of models hyperparameters ,n_iter : the number of random combination set the search takes place,cv: crossvalidation which is used to train on different chunk and test on one chunk  ,n_jobs:use all the cpu,verbose: dont print all the data into console just print the required level of verbose.Now we use random search and randomforest to find the best params and train the model,it took me about 6-7 mins and then the model got trained now i printed the best_params that were : {'n_estimators': 300, 'min_samples_split': 7, 'min_samples_leaf': 8, 'max_depth': 3, 'criterion': 'gini'} . Now i import pickle which is used to save comples python object/Serialization and in this project we use it to save randomforest model while saving i encoutnered an issue where i used rb instead of wb where rb stands for read binary and wb stands for write binary and wb creates a file and writes to it even if it does not exists . now we load the pickle file and use sklearn.metrics and import classification_reports to do model_evaluation find the overall performance and prints it in tabular form.The table contains Accuracy : overall percentage of correct predictions,precision : of all the customers the model predicted Yes what percentage is actually truly Yes, this is the no false positives metric,Recall: this is the no false negatives metric ,F-1 score : the balanced average of precisoin and recall.With this we are done with the experiment part.

So i encountered a problem where i acidentally pushed pickle file to github which i am not supposed to so i used git ls-files | grep .pkl to find the pkl path and then used  git rm --cached notebook/rf_model.pkl to remove pkl from github and then added *.pkl to .gitignore and then committed
---------------------------------------------------------------------------------------------------------------
                                                                                                                                                                DATA INGESTION
src/constants/init.py
We start this section and future ones with the goal to convert experiment into a pipeline in this section we focus on Data ingestion part i initialise all the constants that are necessary for this part for this we go to src folder and then open constants folder and init.py for this we import os and datetime and we initialise DATABASE_NAME , COLLECTION_NAME , MONGODB_URL_KEY.This file includes GobalConfigs (dtabase artifact directory and target column) , AWS config ,Data Ingstion(where the raw data will be stored and the 25% test split ration),Data validation(name for your data drift report ,Data transformation(the directories for your cleaned data and your saved preprocessing.pkl)),Model Trainer(holds the best hyperparameters foind in note book),Model Evaluation & Pusher,App congig(fastAPI configurations) For the data ingestion part the constants that we use are MONGO_DB constants , DATA_INGESTION_COLLECTION_NAME DATA_INGESTION_DIR_NAME ,DATA_INGESTION_FEATURE_STROE_DIR ,DATA_INGESTION_INGESTED_DIR,DATA_INGESTION_TRAIN_TEST_SPLIT_RATIO 

src/configuration/mongo_db_connection.py
now we move on to next one that is establishing mongoDb connection we open the configuration and write the mongoDB conenction code in that file where we import all necessary modules like MyException , logging and Constants(DATABASE_NAME and MONGODB_URL_KEY) this part of code establishes the huge lumpsum of a task that uses lot of time and resources i dont want each of my component (DV,DI) to open its own seperate connection thats inefficient and can  overload the database this class acts as a central gatekeeper ,we name this gatekeeper as client initialised to None and then it proceeeds to create the connection ang becomes a pymongo.MongoCLeint object it takes mongoDB url key from env and then we setup the instance variables we put this everything inside a try catch block so that exceptions can be handled properly.A new thing to notive here is cs = certifi.where() which is a crucial bit of code for fixing avery common SSLError which will be identified by pymongo and will beuseful when connecting to mongoDB which requires SSL .tlsCAFile where tls stands for transport layer security and CAFile stnds for Certificate Authority FIle when you pass it to pymongo you are explicitly tellng to use this specific file as your officila list to verify

proj1-data.py : src/data_access
Now we open Data access folder rom src and open proj1_data.py file which is a dedicated data access layer after MongoDb connection is established its only job is to be the bridge between Database and my local python this code has the contructor that creates an instance of my MongoDBClient(gatekeeper clinet) and another method called exporty collection as dataframe which is the main workhorse of the class it takes a collection_name as an argument and fetches all documents from MongoDb and convert it is python list of dictionaries and then instanrl converst that list of dictionaries to clean pandas dataframe and then the id column is dropped away also it converts any text string 'na' and converty it to np.nan which pandas considers as missing values and then returns the dataframe.Also if you do not provide any database it uses the default one (if block) of the mongo_client that we created in prev section if you provide the database name then it uses else block to find the collection

src/entity/config_entity.py 
I take the raw,static constants from my constants.py file and use them to vuild dynamic organized blueprints for my pipeline @dataclass is a decorator that automatically generates magic methods for my clas like __init__ __repr__(for printing) and __eq__ its a short way to wrtie the class .i also used TIMESTAMP so that the moment the script is imported i can create a timestamp that will be unique ID to every single file and folder for this specific pipeline run.This file also contains the root folder for all outputs(artifacts) from this one pipeline run this is handled by TrianingPipeline Config class. The DataInjestionConfig defines all the paths and settings for the Data iNjestion  step so that all the conponenets are neatly organized into main run folder
data_ingestion_dir will create artifact/timestampe/dta_ingestion and inside data_ingestion fodler feature_store_path will store  raw data.csv file ingsted wil give output and store train.csv and test.csv files ,collection_name is just a string not a path,

src/entity/artifact_entity.py 
this file for now will only contain the code for Dataingestion part which contains the actual results where as config_entity.py will contain the BluePrint and tela a component(like Data ingestion) what to do and where to save theresults ,its the input for the component whereas artifact_entity is the output of the component Thus artifact_entity stores the file paths ofthe outputs from one pipleline step so they can be passed as inputes to the next step(DATA validation).Data injestion produces train.csv and test.csv it returns a DataInGestionArtifact with the paths to those two files which is received by next datavalidation step

src/component/data_ingestion.py
The engine that uses all the above things that we creatd so far. its constructor job is to create DataIngestionConfig object this object contains all the timestamped file paths the collection name and the train-test split ratio and saves all this to instance varialbe self.data_ingestion_config it contains three methods 1)export_data_into_feature_store which uses proj1Data class to connect MongoDB and run the export_collection_as_dataframe() which returns the list of dictionary of data and it converts it to pandas dataframe it gets the feature_store_file_path and creates the directory to store the raw csv file  and then at the end it return the dataframe 2)split_data_as_train_test() takes the raw dataframe from the previous step gets the train_teest_split_ratio from the config uses sklearn to split the data into train_set and test_set it gets the output paths training_file_path and testing_file_path from its config saves the train_set to .../ingested/train.csv and the test_set to .../ingested/test.csv and 3)initiate_data_ingesiton this method will run all the above craeted methods and will return the DataIngestionArtifact object that will be  used by future components

src/pipeline/training_pipeline.py
the constructor method is the setup it immediately buils all the configuration objects and stores them as for now we create an object of the first config i.e., data_ingestion_config ,now we create another method called start_data_ingestion it creates an instance of my DataIngestion which takes the config as an arguement and it calls the method initiate_data_ingestion() which return the data_ingestion_artifact and another method run_pipeline() which is the main function that will execute entire training_pipeline(). 

Now we run the data_ingstion pipeline during the execution i got an error because of not setting the MONGODB_URL as env variable which made the pipeline to raise exception so we fix this by adding the mongoDBURL as env to set a env that can be put forever we use .Zshrc for that we open Zsh using nano ~/.zshrc command and add the variable at the very bottom line
---------------------------------------------------------------------------------------------------------------



